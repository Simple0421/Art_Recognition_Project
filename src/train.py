import torch
import torch.nn as nn
import torch.optim as optim
import os
import sys
import random
import numpy as np
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler  # [å„ªåŒ–] ä½¿ç”¨å…§å»ºçš„ AMP åŠ é€Ÿ

# å°‡ç›®å‰æª”æ¡ˆæ‰€åœ¨çš„ä¸Šä¸€å±¤ç›®éŒ„ (å°ˆæ¡ˆæ ¹ç›®éŒ„) åŠ å…¥ Python æœå°‹è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from dataset import get_dataloaders
from model import get_model

# [å„ªåŒ–] å›ºå®šäº‚æ•¸ç¨®å­ï¼Œç¢ºä¿å¯¦é©—çµæœä¸€è‡´
def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False 

def train_one_epoch(model, loader, criterion, optimizer, device, scaler):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    loop = tqdm(loader, desc="Training", leave=False)
    
    for images, labels in loop:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        
        # [å„ªåŒ–] é–‹å•Ÿæ··åˆç²¾åº¦ (æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨ï¼ŒåŠ å¿«é€Ÿåº¦)
        with autocast():
            outputs = model(images)
            loss = criterion(outputs, labels)
        
        # [å„ªåŒ–] ä½¿ç”¨ Scaler é€²è¡Œåå‘å‚³æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        loop.set_postfix(loss=loss.item())
        
    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def validate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def main():
    # 0. å›ºå®šç¨®å­
    seed_everything(42)
    
    # 1. æº–å‚™è³‡æ–™
    print("æ­£åœ¨è®€å–è³‡æ–™...")
    train_loader, val_loader, class_names = get_dataloaders(
        config.DATA_DIR, 
        batch_size=config.BATCH_SIZE,
        val_split=config.VAL_SPLIT
    )
    
    num_classes = len(class_names)
    print(f"é¡åˆ¥æ•¸é‡: {num_classes}")
    
    # 2. æº–å‚™æ¨¡å‹
    print(f"æ­£åœ¨å»ºç«‹æ¨¡å‹: {config.MODEL_NAME} (å¾®èª¿æ¨¡å¼)...")
    device = torch.device(config.DEVICE)
    model = get_model(num_classes, model_name=config.MODEL_NAME, tune_backend=True).to(device)
    
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    optimizer = optim.Adam(trainable_params, lr=config.LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.NUM_EPOCHS)
    
    # [å„ªåŒ–] åˆå§‹åŒ– Scaler (çµ¦ AMP ç”¨)
    scaler = GradScaler()

    # [å„ªåŒ–] è¨­å®š Early Stopping åƒæ•¸
    patience = 5  # å®¹å¿å¹¾å€‹ Epoch æ²’æœ‰é€²æ­¥
    counter = 0   # ç›®å‰ç´¯ç©å¹¾æ¬¡æ²’é€²æ­¥
    best_acc = 0.0
    
    print(f"é–‹å§‹è¨“ç·´ï¼Œå…± {config.NUM_EPOCHS} å€‹ Epochs...")
    
    for epoch in range(config.NUM_EPOCHS):
        # è¨“ç·´èˆ‡é©—è­‰
        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler)
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        
        # æ›´æ–°å­¸ç¿’ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # é¡¯ç¤ºçµæœ
        print(f"Epoch [{epoch+1}/{config.NUM_EPOCHS}] "
              f"LR: {current_lr:.6f} | "
              f"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | "
              f"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%")
        
        # [å„ªåŒ–] å„²å­˜æœ€ä½³æ¨¡å‹èˆ‡ Early Stopping åˆ¤æ–·
        if val_acc > best_acc:
            best_acc = val_acc
            counter = 0 # é‡ç½®è¨ˆæ•¸å™¨
            if not os.path.exists(config.CHECKPOINT_DIR):
                os.makedirs(config.CHECKPOINT_DIR)
            torch.save(model.state_dict(), config.MODEL_SAVE_PATH)
            print(f"ğŸš€ ç™¼ç¾æœ€ä½³æ¨¡å‹ (Acc: {best_acc:.2f}%)ï¼Œå·²å„²å­˜ã€‚")
        else:
            counter += 1
            print(f"âš ï¸ Validation Accuracy æœªæå‡ ({counter}/{patience})")
            if counter >= patience:
                print("ğŸ›‘ è§¸ç™¼ Early Stoppingï¼Œææ—©çµæŸè¨“ç·´ã€‚")
                break
                
    print("è¨“ç·´çµæŸï¼")

if __name__ == "__main__":
    main()